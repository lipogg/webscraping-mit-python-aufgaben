{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Abgabe bis 04.01.2025, 23:59 Uhr per Mail: l.poggel@fu-berlin.de**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Aufgabe 1: Wiederholung\n",
    "&nbsp;"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lest euch nochmal das Kapitel \"APIs\" auf unserer [Kurs-Website](https://lipogg.github.io/webscraping-mit-python/) sowie die beiden Beispiele zur DraCor- und zur LOC-API durch, achtet dabei insbesondere auf die Beispiele zu den drei verschiedenen Rate Limiting Strategien. Formuliert eine Frage zu einem Inhalt, der euch noch nicht ganz klar ist."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Aufgabe 2: Verständnis\n",
    "&nbsp;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Der folgende Code stellt drei Anfragen an die QuotesToScrape-Seite. Dabei wird die Abfragerate nicht kontrolliert. Schreibt den Code mithilfe von Python Dekoratoren so um, dass nur eine Anfrage alle 20 Sekunden gestellt wird. Hinweis: Macht euch zunächst klar, was der Code eigentlich macht. Vergleicht dann den Aufbau des Codes mit dem Schema unter [\"2. Python Dekoratoren aus dem Paket ratelimit\"](https://lipogg.github.io/webscraping-mit-python/chapters/08/subchapters/03_loc_api.html#rate-limits-berucksichtigen-und-die-abfragerate-steuern). Schreibt danach den Code so um, dass der Aufbau dem Schema entspricht. Fügt zuletzt Python Dekoratoren hinzu, um die Anfragerate zu verringern. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import requests\n",
    "\n",
    "def get_status_codes(urls):\n",
    "    codes_list = [requests.get(url).status_code for url in urls]\n",
    "    return codes_list\n",
    "\n",
    "request_urls = [\"https://quotes.toscrape.com/page/1\", \"https://quotes.toscrape.com/page/4\", \"https://quotes.toscrape.com/page/7\"]\n",
    "status_codes = get_status_codes(request_urls)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Aufgabe 3: Praxis\n",
    "&nbsp;"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Hinweis: Bei der Bearbeitung dieser Aufgaben könnt ihr euch nach dem LOC-Beispiel auf der Kurs-Website richten."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "1.) Öffnet euer Terminal bzw. Anaconda Prompt. Navigiert mit dem Befehl `cd` in euren Projektordner. Aktiviert die virtuelle Umgebung, startet JupyterLab und wechselt in das Browserfenster, in dem JupyterLab geöffnet wurde. Erstellt ein neues Jupyter Notebook, indem ihr auf den Kernel \"Python 3.12 (Webscraping)” klickt. Verfasst einen Kommentar mit dem Namen der Lehrveranstaltung, der Nummer des Übungsblatts und eurem Namen. Speichert das Notebook mit einem geeigneten Dateinamen ab."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "2.) In der letzten Stunde haben wir uns die Chronicling America API der Library of Congress angesehen. In dieser Übung werdet ihr euch mit der loc.gov-API beschäftigen. Lest euch die Dokumentationsseite https://www.loc.gov/apis/json-and-yaml/ durch. Welche Daten stellt die API bereit? "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**Im Folgenden sollt ihr mithilfe der loc.gov-API alle Bilder von Suffragetten aus dem \"Prints and Photographs Online Catalog\" der LOC abfragen und in einem neuen Verzeichnis auf eurem Computer speichern. Wenn ihr herausgefordert werden wollt, bearbeitet die Aufgabe selbstständig. Wenn ihr etwas Hilfestellung benötigt, bearbeitet nacheinander die folgenden Teilaufgaben.** "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.) Importiert zunächst die erforderlichen Pakete."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "4.) Seht euch die Dokumentation zur loc.gov-API auf der Seite https://www.loc.gov/apis/json-and-yaml/requests/ an. Wie muss die Basis-URL für die Abfragen aussehen? Erstellt eine neue Variable `url`, der ihr die Basis-URL als F-String zuweist. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "5.) Lest euch die Dokumentation zu den API Endpunkten unter https://www.loc.gov/apis/json-and-yaml/requests/endpoints/ durch. Welchen Endpunkt müsst ihr verwenden, wenn ihr den Katalog bei der Abfrage nach bestimmten Suchbegriffen durchsuchen wollt?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "6.) Fügt jetzt dem F-String mit der Basis-URL zwei Suchparameter in der korrekten Syntax hinzu, und zwar \"?q={searchterm}\" und \"&fo={format}\". Zuletzt erstellt ihr drei Variablen: Eine Variable `endpoint`, welcher ihr den String \"pictures/search\" zuweist, eine Variable `searchterm`, welcher ihr den Suchbegriff \"suffragettes\" zuweist, und eine Variable `response_format`, welcher ihr den String \"json\" zuweist. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "7.) Führt eine HTTP GET-Anfrage für die soeben zusammengesetzte URL durch, wandelt die Antwort in ein Python Dictionary um und weist das Dictionary einer Variable zu. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "8.) Seht euch das Dictionary genau an. Unter welchem Schlüssel befinden sich die Suchergebnisse? Wie sind die Suchergebnisse organisiert? Wie viele Suchergebnisse hat die GET-Anfrage geliefert? Sind das alle Suchergebnisse oder nur die Ergebnisse von der ersten Ergebnisseite? Wie viele Ergebnisseiten gibt es insgesamt? Überprüft eure Antworten, indem ihr unter https://loc.gov/pictures/ nach \"suffragettes\" sucht und die Ergebnisse der Suche über die Suchmaske mit den Ergebnissen aus der API-Abfrage vergleicht."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9.) Greift über den entsprechenden Schlüssel auf die Suchergebnisse zu und weist sie einer neuen Variable zu. Seht euch die ersten drei Suchergebnisse genauer an. Welcher Schlüssel enthält eine eindeutige Identifikationsnummer? Im Folgenden werdet ihr diese Identifikationsnummer zur Erstellung von eindeutigen  Dateinamen verwenden. Seht euch die Suchergebnisse noch einmal an. Welcher Schlüssel enthält einen Link zu der gefundenen Bilddatei in möglichst guter Auflösung? Wie kann man darauf zugreifen?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10.) Schreibt eine for-Schleife, die durch die Variable mit den Suchergebnissen iteriert. In jedem Schleifendurchlauf soll zunächst die Identifikationsnummer des aktuellen Suchergebnisses einer neuen Variable `image_id` zugewiesen werden. Danach soll der Link zu der Bilddatei in der vollen Größe einer weiteren Variable `image_url` zugewiesen werden. Danach soll eine HTTP GET-Anfrage für diese URL durchgeführt werden und der Inhalt der Antwort (Attribut `content`) soll einer neuen Variable `image` zugewiesen werden. Zuletzt soll das Bild in einer neuen Datei gespeichert werden. Beim Öffnen der Datei soll als Dateiendung \".jpg\" festgelegt werden und als Zugriffsmodus \"wb\". "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11.) Legt jetzt ein neues Verzeichnis an, in dem ihr später die Bilder der Suffragetten abspeichern werdet."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Im Folgenden soll die for-Schleife aus Teilaufgabe 10 für alle Ergebnisseiten reproduziert werden. Überlegt zunächst selbst, wie man dabei vorgehen könnte und formuliert in ein paar Worten einen Lösungsansatz. Versucht, euren Lösungsansatz umzusetzen. Wenn das nicht klappt, könnt ihr stattdessen meinem Lösungsansatz in den Teilaufgaben 13 und 14 folgen.**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "12.) Schaut euch zunächst noch einmal das Python Dictionary zur allerersten GET-Anfrage aus Teilaufgabe 7 und eure Antwort zu Teilaufgabe 8 an. Welcher Schlüssel enthält als Wert die Gesamtanzahl der Ergebnisseiten? Greift auf diesen Wert zu und weist ihn einer neun Variable `total_pages` zu."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "13.) Seht euch den Schlüssel \"page_list\" mit den Ergebnisseiten an. Jede Ergebnisseite hat einen Zusatzparameter `sp` mit der aktuellen Seitenzahl. Fügt diesen Parameter an eure Abfrage-URL an. Erstellt eine for-Schleife, die alle Ergebnisseiten durchläuft. In jedem Schleifendurchlauf soll zunächst die aktuelle Seitenzahl an die Abfrage-URL angefügt werden. anschließend soll eine HTTP GET-Anfrage für die aktuelle URL durchgeführt werden und die Antwort in ein Python Dictionary umgewandelt werden (Vgl. Teilaufgabe 7). Danach sollen die Suchergebnisse einer neuen Variable zugewiesen werden (Vgl. Teilaufgabe 9). Zuletzt soll das Objekt mit den Suchergebnissen durchlaufen werden und jedes Bild soll in einer Datei mit der Identifikationsnummer als Dateiname in dem in Teilaufgabe 11 angelegten Verzeichnis gespeichert werden (Vgl. Teilaufgabe 10). Führt die Schleife noch nicht aus."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "14.) Bevor ihr den Code aus Teilaufgabe 13 ausführt, müsst ihr noch die Abfragerate an die Vorgaben der LOC anpassen. Die erlaubte Abfragerate könnt ihr dem Abschnitt \"Collections, format, and other endpoints\" auf der Seite https://www.loc.gov/apis/json-and-yaml/working-within-limits/ entnehmen. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "15.) Führt zuletzt den Code aus Teilaufgabe 13 aus und schaut euch die heruntergeladenen Dateien an. Hat alles geklappt? "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Achtung:** In dem neuen Verzeichnis liegen am Ende 283 und nicht 282 Objekte. Das liegt daran, dass bei der Arbeit mit Jupyter Notebooks automatisch ein unsichtbares Verzeichnis angelegt wird, das .ipynb_checkpoints heißt. Falls ihr euch also die Anzahl der Dateien in eurem Ordner anzeigen lasst und über die Zahl 283 stolpert, wundert euch nicht: Das liegt an dem versteckten Verzeichnis. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "16.) Erläutert zuletzt, was aus eurer Sicht die Vor- und Nachteile sind, wenn man Identifikationsnummern anstelle von Titel und Datum als Dateinamen wählt: Welche Probleme könnten auftauchen, wenn die Werte der Schlüssel \"title\" und \"created_published_date\" statt \"pk\" als Dateinamen verwendet werden? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "#### Aufgabe 4: Vorblick\n",
    "&nbsp;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lest den neuen Abschnitt [\"9.1 Statisch vs. Dynamisch\"](https://lipogg.github.io/webscraping-mit-python/chapters/09/subchapters/01_statisch_vs_dynamisch.html) auf der Kurswebsite. \n",
    "\n",
    "Öffnet anschließend die Webseite, die ihr im Rahmen von eurer Projektarbeit scrapen wollt, im Chrome Browser. Stellt über die Entwicklerwerkzeuge JavaScript aus und ladet die Seite neu. Werden die Inhalte, die ihr extrahieren wollt, mithilfe von JavaScript geladen und dargestellt? Gibt es vielleicht eine API, welche Zugriff auf die Daten ermöglicht? "
   ]
  }
 ],
 "metadata": {
  "authors": [
   {
    "name": "Lisa Poggel"
   }
  ],
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "subtitle": "WiSe 2024/25 - 'Webscraping mit Python'",
  "title": "Übungsblatt 9"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
