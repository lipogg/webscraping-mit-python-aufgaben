{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Abgabe bis 25.01.2025, 23:59 Uhr per Mail: l.poggel@fu-berlin.de**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Aufgabe 1: Wiederholung\n",
    "&nbsp;\n",
    "\n",
    "Lest euch nochmal die Inhalte im neuen Abschnitt \"Fortsetzung Selenium\" auf der Kurs-Website durch. Formuliert eine Frage zu einem Inhalt, der euch noch nicht ganz klar ist."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Aufgabe 2: XPath\n",
    "&nbsp;"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "1.) Schaut euch das folgende Video an (etwa 15 min.): \n",
    "[https://www.youtube.com/watch?v=0QHmDvc9abs](https://www.youtube.com/watch?v=0QHmDvc9abs) \n",
    "\n",
    "2.) Schreibt XPath-Ausdrücke zur Suche nach folgenden HTML-Elementen: \n",
    "\n",
    "* Ein div-Element irgendwo im HTML-Baum\n",
    "* Ein a-Element mit einem div-Element als Elternelement. Das div-Element ist irgendwo im HTML-Baum\n",
    "* Ein img-Element mit dem Attribut \"src='https://example.com'\", irgendwo im HTML-Baum\n",
    "* Ein button-Element mit dem Text \"Hier klicken\", irgendwo im HTML-Baum\n",
    "\n",
    "Zur Bearbeitung dieser Aufgabe könnt ihr euch nach den Beispielen im Video richten und zum Beispiel die folgenden Ressourcen nutzen:\n",
    "\n",
    "* Tutorial in Textform mit Schaubild und Beispielen: [https://www.softwaretestinghelp.com/xpath-writing-cheat-sheet-tutorial-examples/](https://www.softwaretestinghelp.com/xpath-writing-cheat-sheet-tutorial-examples/)\n",
    "* Liste der wichtigsten XPath Ausdrücke: [https://www.w3schools.com/xml/xpath_syntax.asp](https://www.w3schools.com/xml/xpath_syntax.asp)\n",
    "* XPath Cheatsheet: [https://devhints.io/xpath](https://devhints.io/xpath)\n",
    "* XPath Tester: [https://extendsclass.com/xpath-tester.html](https://extendsclass.com/xpath-tester.html)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Aufgabe 3: Selenium\n",
    "&nbsp;\n",
    "\n",
    "Diese Übungsaufgabe widmet sich der Website https://www.pinterest.com/, genauer gesagt der Unterseite https://www.pinterest.com/ideas/animals/925056443165/. Überprüft als erstes die robots.txt. Dürft ihr die Seite überhaupt scrapen?\n",
    "\n",
    "1.) Ruft im regulären Chrome-Browser die Seite [https://www.pinterest.com/ideas/animals/925056443165/](https://www.pinterest.com/ideas/animals/925056443165/) auf. Unter der Überschrift \"Find Animals ideas\" (bzw. \"Ideen wie diese\") seht ihr eine Reihe Kacheln mit verschiedenen Fotos. Im Folgenden sollt ihr die Links zu allen Fotos unter dieser Überschrift extrahieren, also Links der Art \"https://i.pinimg.com/236x/83/13/be/8313bedcf4f275b4b60986a347f57c5f.jpg\". \n",
    "\n",
    "Überprüft zunächst mithilfe der Browser-Entwicklertools: Wird zur Darstellung der Kacheln mit den Tierfotos tatsächlich JavaScript verwendet? \n",
    "\n",
    "2.) Nachdem ihr in Teilaufgabe 1 erfahren habt, dass JavaScript bei der Darstellung der Kacheln mit den Tierfotos beteiligt ist, wisst ihr, dass ihr zur Lösung der Aufgabe Selenium verwenden müsst. Dazu sollt ihr zunächst ein paar Fragen klären, indem ihr die Webseite und den Quellcode untersucht:\n",
    "\n",
    "- Navigiert in den Entwicklertools durch die HTML-Baumstruktur zu einer der Kacheln unter der Überschrift \"Find Animals ideas\". Notiert euch die CSS-Klasse aus dem class-Attribut des Elements, welches zur Darstellung der Kachel verwendet wird. Sucht mit Strg + F (bzw. Cmd + F) in den Entwicklertools nach dieser Klasse. Welche anderen Elemente haben dieselbe Klasse? Ist das class-Attribut geeignet, um die Kacheln unter der Überschrift \"Find Animals ideas\" eindeutig zu identifizieren? \n",
    "- Navigiert in den Entwicklertools nun durch das HTML-Element, das zur Darstellung der Kachel verwendet wird, bis ihr das Element mit dem Link zur Bilddatei findet. Wie heißt das Element? Wie heißt das Attribut mit dem Link? Notiert euch auch diese Information. \n",
    "- Scrollt runter bis zum Seitenende. Sind bereits beim Seitenaufruf alle Kacheln geladen oder werden Inhalte erst beim Scrollen geladen? \n",
    "- Schreibt zuletzt einen XPath-Ausdruck, der alle Elemente mit den Links zu den gesuchten Tierfotos findet. \n",
    "\n",
    ">Hinweis: Ihr könnt nach dem Schema `//irgendein_element//noch_ein_element` nach einem Element suchen, dass sich irgendwo in einem anderen Element befindet. \n",
    "\n",
    "3.) Schreibt mit den Informationen aus Teilaufgabe 2 jetzt einen Webscraper, der die Links zu allen Tierfotos extrahiert und in einer Liste speichert. Dabei könnt ihr zunächst genauso vorgehen wie im Beispiel auf der Kurswebsite. Denkt dabei daran, beim Scrollen Wartezeiten einzubauen, entweder mit time.sleep() oder Selenium Waits. In der letzten Stunde habt ihr zwei verschiedene Scrollstrategien kennengelernt: Eine Strategie, bei der jedesmal um eine festgelegte Pixelanzahl gescrollt wird, und eine Strategie, bei der jedes Mal um die gesamte Höhe des angezeigten Seitenbereichs gescrollt wurde. Zum Scrollen könnt ihr eine beliebige der beiden Scrollstrategien verweden. \n",
    "\n",
    "Ihr müsst zur Lösung der Aufgabe die folgenden Zeilen umschreiben: \n",
    "\n",
    "* `unterkuenfte = driver.find_elements(By.XPATH, \"//div[@data-testid='listing-card-title']\")`\n",
    "\n",
    "Setzt hier den XPath-Ausdruck aus Teilaufgabe 2 ein. \n",
    "\n",
    "* `unterkuenfte_orte = [unterkunft.text for unterkunft in unterkuenfte]`\n",
    "\n",
    "In dieser Zeile habe ich list comprehension verwendet, um den Text aus jedem HTML-Element in der Liste `unterkuenfte` zu extrahieren. In der Stunde haben wir stattdessen eine for-Schleife geschrieben, die genau dasselbe macht. In unserem Pinterest-Beispiel wollen wir aber jetzt nicht den Text extrahieren, sondern den Wert des Attributs, das den Link zur Bilddatei enthält. Das geht in Selenium mit der Methode `.get_attribute(\"name_des_attributs\")`. Also:\n",
    "\n",
    "`unterkuenfte_orte = [unterkunft.get_attribute(\"name_des_attributs\") for unterkunft in unterkuenfte]`\n",
    "\n",
    "Mehr zur Methode `.get_attribute()` in den Selenium-Dokumentationsseiten: \n",
    "[https://www.selenium.dev/documentation/webdriver/elements/information/#fetching-attributes-or-properties](https://www.selenium.dev/documentation/webdriver/elements/information/#fetching-attributes-or-properties)\n",
    "\n",
    "Natürlich solltet ihr auch passende Variablenamen wählen. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "5.) Lasst euch die Liste mit den extrahierten Bilderlinks ausgeben. Es fällt auf: Die Liste enthält nicht alle Links, sondern nur die Links zu den Tierfotos am Ende der Seite! Woran könnte das liegen? Um dem Problem auf den Grund zu gehen, untersucht das Scrollverhalten mithilfe der Entwicklertools: Sucht mit Strg + F nach dem XPath-Ausdruck, den ihr zur Suche nach den Elementen mit den Links zu den Tierfotos verwendet habt, scrollt ein Stück weit runter, und sucht erneut mit Strg + F. Es sollte dabei deutlich werden, dass immer nur die Kacheln gefunden werden, die sich im aktuellen Sichtbarkeitsbereich der Webseite befinden. Auf dieser Webseite werden also die Elemente nicht dauerhaft in das HTML-Gerüst eingefügt wie auf der Seite airbnb.com, sondern nur temporär, und zwar so lange, wie ein Element im Browser angezeigt wird. Elemente können also nur dann gefunden werden, wenn sie sich im aktuell sichtbaren Seitenbereich befinden, und nicht mehr, nachdem bis zum Ende der Seite gescrollt wird. Überlegt zunächst selbst: Wie muss der Code aus Teilaufgabe 4 angepasst werden, damit alle Links extrahiert werden, und nicht nur die Links vom Seitenende? \n",
    "\n",
    ">Hinweis: Falls ihr nicht darauf kommt: Die Links müssen im Körper der while-Schleife extrahiert werden, und nicht erst nachdem die Schleife terminiert ist. Achtet dabei darauf, dass ihr vor dem Beginn der while-Schleife bereits ein leere Liste für die extrahierten Links erstellt, die ihr in jedem Schleifendurchlauf befüllt. \n",
    "\n",
    "6.) Führt den Code erneut aus und lasst euch die Liste mit den Links ausgeben. Wie viele Links wurden jetzt gefunden? \n",
    "\n",
    "> Hinweis: Um herauszufinden, wie viele Links gefunden wurden, könnt ihr euch einfach mit len() die Länge der Liste ausgeben lassen. \n",
    "\n",
    "7.) Wandelt die Liste in ein Set um, um Duplikate zu entfernen, und lasst euch erneut die Anzahl der Links ausgeben. Die Liste enthält sowohl Duplikate von Bildern, die beim Scrollen zweimal im Sichtbarkeitsbereich waren (z.B. erst die eine Hälfte, dann die andere Hälfte), als auch Bilder, die tatsächlich mehrmals geposted wurden. Je nachdem, welche Scrollmethode ihr verwendet habt, enthält eure Liste deswegen unterschiedlich viele Duplikate, die beim Umwandeln in ein Set entfernt werden. Welche der beiden Scrollstrategien eignet sich besser, um eine möglichst geringe Anzahl von Duplikaten zu bekommen? \n",
    "\n",
    "8.) Überlegt selbst: Kann mit den extrahierten Daten eine Forschungsfrage der Art \"Wie häufig wird Bild XY geposted\" beantwortet werden? \n"
   ]
  }
 ],
 "metadata": {
  "authors": [
   {
    "name": "Lisa Poggel"
   }
  ],
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "subtitle": "WiSe 2024/25 - 'Webscraping mit Python'",
  "title": "Übungsblatt 11"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
