{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Abgabe bis 09.02.2025, 23:59 Uhr per Mail: l.poggel@fu-berlin.de**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Aufgabe 1: Wiederholung\n",
    "&nbsp;\n",
    "\n",
    "Lest euch nochmal die Inhalte im Abschnitt \"Daten visualisieren\" im Kapitel \"Pandas (Exkurs)\" sowie die Inhalte im neuen Kapitel \"Tipps und Tricks\" auf der Kurs-Website durch. Formuliert eine Frage zu einem Inhalt, der euch noch nicht ganz klar ist.\n",
    "\n",
    "Ihr könnt auch Fragen formulieren, die sich auf irgendeinen anderen Inhalt aus dem Seminar oder zu meinen Korrekturen (bekommt ihr bis Sonntag) beziehen. Diese Fragen können wir in der letzten Stunde hoffentlich noch klären."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Aufgabe 2: Verständnis Sessions \n",
    "&nbsp;\n",
    "\n",
    "1.) Beantwortet kurz die folgenden Fragen: \n",
    "\n",
    "- Was ist eine Session? \n",
    "- Was hat es für einen Vorteil, wenn wiederholte Anfragen aus derselben Sessions heraus gestellt werden? \n",
    "\n",
    "2.) Wie könnte der folgende Code aus den Sitzungen zum Webscrapen mit BeautifulSoup am Anfang des Semesters umgeschrieben werden, sodass alle Anfragen aus derselben Session heraus gestellt werden? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = \"http://quotes.toscrape.com\"\n",
    "current_url = base_url\n",
    "quotes = []\n",
    "\n",
    "while True:\n",
    "    page = requests.get(current_url)\n",
    "    soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "    for quote in soup.select(\".quote > .text\"):\n",
    "        quotes.append(quote.text)\n",
    "\n",
    "    next_elem = soup.select_one(\".next > a\")\n",
    "\n",
    "    if next_elem is not None:\n",
    "        next_path = next_elem['href'] # auf den Wert des Attributs \"href\" zugreifen\n",
    "        current_url = base_url + next_path\n",
    "    else:\n",
    "        break"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Aufgabe 3: Rundumschlag \n",
    "&nbsp;\n",
    "\n",
    "1.) Untersucht die folgenden drei Webseiten und entscheidet euch, welche der vorgestellten Methoden zur Extraktion der genannten Daten verwendet werden sollte (d.h. Webscraping mit BeautifulSoup, Webscraping mit Selenium, API-Abfrage oder Direktdownload über den Netzwerktab).\n",
    "\n",
    "- Buchtitel und Anzahl der Votes für alle Bücher auf der Seite https://www.goodreads.com/choiceawards/readers-favorite-science-fiction-books-2024\n",
    "- Titel und Autor:innen aller Beiträge auf der Seite https://specials.dekoder.org/, für alle Sprachversionen, in denen die Website verfügbar ist (die Sprachen seht ihr oben rechts)\n",
    "- Die 100 Top Posts auf Bluesky, die das Wort \"election\" enthalten: https://bsky.app/\n",
    "\n",
    "2.) Sucht euch eine der drei Webseiten aus und extrahiert die genannten Inhalte mit der gewählten Strategie. Achtung: beim Scrapen von goodreads.com muss "
   ]
  }
 ],
 "metadata": {
  "authors": [
   {
    "name": "Lisa Poggel"
   }
  ],
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "subtitle": "WiSe 2024/25 - 'Webscraping mit Python'",
  "title": "Übungsblatt 13"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
