{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Abgabe bis 12.01.2024, 23:59 Uhr per Mail: l.poggel@fu-berlin.de**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Diese Übungsaufgaben widmen sich zwei Webseiten, die auf den ersten Blick ähnlich aussehen: https://minimalistbaker.com/recipe-index/ und https://www.airbnb.com/. Der Lerneffekt soll sein, dass ihr lernt einzuschätzen, welche Webscraping-Strategie für verschiedene Webseiten verwendet werden können."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Aufgabe 1: Wiederholung\n",
    "&nbsp;\n",
    "\n",
    "Lest euch nochmal die Inhalte im neuen Kapitel \"Dynamische Webseiten\" auf der Kurs-Website durch. Formuliert eine Frage zu einem Inhalt, der euch noch nicht ganz klar ist."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Aufgabe 2: Praxisaufgabe BeautifulSoup\n",
    "&nbsp;"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "1.) Öffnet euer Terminal bzw. Anaconda Prompt, navigiert in eueren Projektordner, aktiviert die virtuelle Umgebung und startet JupyterLab. Erstellt ein neues Jupyter Notebook und verfasst einen Kommentar mit dem Namen der Lehrveranstaltung, der Nummer des Übungsblatts und eurem Namen. Speichert das Notebook mit einem geeigneten Dateinamen ab."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "2.) Diese Übungsaufgabe widmet sich der Wordpress-Seite https://minimalistbaker.com/. Die Seite habe ich auf der Kurswebsite als Beispiel verwendet. Überprüft zunächst, ob ich euch keinen Quatsch erzählt habe: Gebt die Website-URL auf der Seite https://www.isitwp.com/ ein. Verwendet die Website wirklich WordPress?\n",
    "\n",
    "3.) Überprüft als nächstes die robots.txt. Dürft ihr die Seite überhaupt scrapen?\n",
    "\n",
    "4.) Ruft die Website https://minimalistbaker.com/ im Chrome-Browser auf. Wenn ihr auf \"All Recipes\" klickt, wird die Seite https://minimalistbaker.com/recipe-index/ aufgerufen. Scrollt zum Seitenende runter, bis ihr die Seitenzahlen seht. Bei https://minimalistbaker.com/recipe-index/ handelt es sich also um die erste von 84 Seiten. Im Folgenden sollt ihr die Namen  von allen Rezepten von einer der 84 Seite extrahieren, und zwar von Seite 2. Was ist die URL zu der Seite?\n",
    "\n",
    "5.) Überprüft, ob zur Darstellung der Kacheln mit den Rezepten JavaScript verwendet wird. Wie kann man das überprüfen?\n",
    "\n",
    "6.) Nachdem ihr in Teilaufgabe 5 erfahren habt, dass JavaScript nicht bei der Darstellung der Kacheln mit den Rezepten beteiligt ist, wisst ihr, dass ihr diese Aufgabe wahrscheinlich mit requests und BeautifulSoup lösen könnt. Dazu sollt ihr zunächst ein paar Fragen klären, indem ihr die Webseite und den Quelltext untersucht:\n",
    "\n",
    "- Welche HTML-Elemente werden für die Darstellung der Kacheln mit den verschiedenen Rezepten verwendet?\n",
    "- Sucht im Quellcode der Seite nach der Überschrift \"Epic Vegan Sweet Potato Pie\". Gibt es ein HTML-Element oder vielleicht sogar ein class-Attribut, das addressiert werden kann, um die Namen aller Rezepten auf der Seite zu extrahieren?\n",
    "\n",
    "7.) Schreibt mit diesem Wissen und mithilfe von BeautifulSoup und requests einen kleinen Web Scraper, der alle Namen von allen Rezepte von der Seite extrahiert. Falls ihr etwas herausgefordert werden wollt, könnt ihr außerdem zu jedem Rezept das Foto extrahieren. \n",
    "\n",
    "\n",
    "#### Aufgabe 3: Praxisaufgabe Selenium\n",
    "&nbsp;\n",
    "\n",
    "1.) Diese Übungsaufgabe widmet sich der Website https://www.airbnb.com/. Überprüft als erstes die robots.txt. Dürft ihr die Seite überhaupt scrapen?\n",
    "\n",
    "2.) Ruft die Website https://www.airbnb.com/ im Chrome-Browser auf. Im Folgenden sollt ihr die Ortsangaben zu den Unterkünften, die auf der Startseite https://www.airbnb.com/ angezeigt werden, scrapen. Scrollt zunächst runter bis zum Seitenende. Klickt auf \"Show more\" und scroll anschließend wieder bis zum Seitenende. Ihr bemerkt: Die Seite verwendet \"infinite scrolling\", sodass es kein festgelegtes Seitenende gibt. Das ist bereits ein Hinweis darauf, dass die Seite JavaScript zur Darstellung der Inhalte verwendet.\n",
    "\n",
    "3.) Überprüft jetzt mithilfe der Browser-Entwicklertools: Wird zur Darstellung der Kacheln mit den Unterkünften tatsächlich JavaScript verwendet?\n",
    "\n",
    "4.) Nachdem ihr in Teilaufgabe 3 erfahren habt, dass JavaScript bei der Darstellung der Kacheln mit den Unterkünften beteiligt ist, wisst ihr, dass ihr diese Aufgabe nicht mit requests und BeautifulSoup lösen könnt. Stattdessen werdet ihr Selenium verwenden. Dazu sollt ihr zunächst ein paar Fragen klären, indem ihr die Webseite und den Quelltext untersucht:\n",
    "\n",
    "- Welche HTML-Elemente werden für die Darstellung der Kacheln mit den verschiedenen Unterkünften verwendet?\n",
    "- Sucht im Quellcode der Seite nach der Überschrift der ersten Kachel. Gibt es ein HTML-Element oder vielleicht sogar ein class-Attribut, das addressiert werden kann, um die Orte aller Unterkünfte auf der Seite zu extrahieren?\n",
    "\n",
    "5.) Schreibt mit diesem Wissen und mithilfe von Selenium und dem Chrome WebDriver einen kleinen Web Scraper, der alle Ortsangaben zu allen Unterkünften von der Seite extrahiert. Dabei könnt ihr grundsätzlich so vorgehen, wie im Beispiel auf der Kurswebsite, aber ihr müsst folgendes beachten:\n",
    "\n",
    "- Bei der Suche mithilfe von CSS-Selektoren (also mithilfe der class-Attribute) müsst ihr Leerzeichen durch einen Punkt ersetzen, falls der CSS-Selektor ein Leerzeichen enthält. Also zum Beispiel müsstet ihr \"g1qv1ctd c1v0rf5q dir dir-ltr\" ändern zu \"g1qv1ctd.c1v0rf5q.dir.dir-ltr\".\n",
    "- Nachdem ihr die HTTP-Anfrage gestellt habt, müsst ihr ein paar Sekunden warten, damit der Browser genug Zeit hat, die Inhalte zu rendern. Eine automatische Wartezeit könnt ihr einstellen mit time.sleep(5). Dazu müsst ihr zunächst wieder das Modul time importieren. In der nächsten Woche lernt ihr eine bessere Methode kennen, um in Selenium auf das Laden von Inhalten zu warten. \n",
    "- Da die Seite infinite scrolling verwendet, findet euer Web Scraper nur eine begrenzte Anzahl an Elementen. Um mehr Elemente zu finden, muss das Scrollen im Browser simuliert werden. Das müsst ihr aber noch nicht umsetzen; das lernen wir nächste Stunde."
   ]
  }
 ],
 "metadata": {
  "authors": [
   {
    "name": "Lisa Poggel"
   }
  ],
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "subtitle": "WiSe 2024/25 - 'Webscraping mit Python'",
  "title": "Übungsblatt 10"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
